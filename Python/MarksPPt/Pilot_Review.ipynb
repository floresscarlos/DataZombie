{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profile = raw_input(\"Profile name (e.g. ethan):\")\n",
    "\n",
    "result_filename = raw_input(\"Company Name (e.g. linkedin):\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if profile == 'ethan' :\n",
    "    template_location = ''\n",
    "    result_location = 'C:\\\\Users\\\\eandrian\\\\Downloads\\\\'\n",
    "    jar_location = 'C:\\\\Users\\\\eandrian\\\\Documents\\\\Jarfolder\\\\daptor-jdbc-0.9.1-standalone.jar'\n",
    "    print ('Loading Ethan\\'s profile')\n",
    "\n",
    "elif profile == 'karmen' :\n",
    "    template_location = 'C:\\\\Users\\\\kachong\\\\OneDrive - Microsoft\\\\Notebooks\\\\Pilot_Review.pptx'\n",
    "    result_location = 'C:\\\\Users\\\\kachong\\\\Downloads\\\\'\n",
    "    jar_location = 'C:\\\\Users\\\\kachong\\\\Documents\\\\jar_files\\\\daptor-jdbc-0.9.1-standalone.jar'\n",
    "    print ('Loading Karmen\\'s profile')\n",
    "\n",
    "elif profile == 'sabina' :\n",
    "    template_location = ''\n",
    "    result_location = 'C:\\\\Users\\\\ssobinin\\\\Downloads\\\\'\n",
    "    jar_location = 'C:\\\\Users\\\\ssobinin\\\\daptor-jdbc-0.9.1-standalone.jar'\n",
    "    print ('Loading Sabina\\'s profile')\n",
    "        \n",
    "else :\n",
    "    print ('Profile not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import getpass\n",
    "import pandas\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "\n",
    "# import libraries\n",
    "\n",
    "username = raw_input(\"Teradata Username:\")\n",
    "\n",
    "password = getpass.getpass(prompt='Teradata password:')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyodbc.pooling = False \n",
    "\n",
    " \n",
    "cxn = pyodbc.connect(\"DRIVER={Teradata};\\\n",
    "DBCNAME=dwprod1.corp.linkedin.com;\\\n",
    "DATABASE=DWH;\\\n",
    "AUTHENTICATION=LDAP;\\\n",
    "UID=%s;\\\n",
    "PWD=%s\"\\\n",
    "%(username,password),\\\n",
    "autocommit = True,\\\n",
    " ANSI = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from linkedin.presto_daptor.presto_setup import get_presto_connection\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas\n",
    "matplotlib.style.use('ggplot')\n",
    "\n",
    "daptor_url = 'jdbc:daptor://ltx1-spidergw01.grid.linkedin.com:44444/hive;ssl=true;'\n",
    "presto_conn = get_presto_connection(daptor_url, jar_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contract = raw_input(\"contract ids (e.g. 'CS2199415-16'):\") \n",
    "\n",
    "dashboard = raw_input(\"dashboard ids (comma separated), optional:\") \n",
    "\n",
    "country = raw_input(\"country_sks (comma separated), optional:\") \n",
    "CS3801856-17\n",
    "continent = raw_input(\"continent_sks (comma separated, optional):\") \n",
    "\n",
    "company_id = raw_input(\"company_id (comma separated, required):\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if inputs are empty. if they are then comment them out of the target sql query\n",
    "\n",
    "if country == \"\" :\n",
    "    country_filter = \"--\"\n",
    "else :\n",
    "    country_filter = \"\"\n",
    "    \n",
    "if continent == \"\" :\n",
    "    continent_filter = \"--\"\n",
    "else : \n",
    "    continent_filter = \"\"\n",
    "    \n",
    "if contract == \"\" :\n",
    "    dashboard_id_filter = \"\"\n",
    "    contract_id_filter = \"--\"\n",
    "    \n",
    "elif contract == \"\" and dashboard == \"\" :\n",
    "    print \"No Contract or Dashboard ID provided\"\n",
    "\n",
    "else :\n",
    "    dashboard_id_filter = \"--\"\n",
    "    contract_id_filter = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Only run after generating first report of this session\n",
    "\n",
    "sql_drop1 = \"\"\"\n",
    "drop table all_sp;\n",
    "\"\"\"\n",
    "\n",
    "sql_drop2 = \"\"\"\n",
    "drop table all_sn;\n",
    "\"\"\"\n",
    "\n",
    "sql_drop3 = \"\"\"\n",
    "drop table combined_sp;\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql0_seats = \"\"\"create volatile table all_sn as (\n",
    "select lss_contract_id\n",
    ", substr(SF_CONTRACT_NUMBER,1,12) AS sf_contract\n",
    ", a.lss_seat_id\n",
    ", a.member_id as member_sk \n",
    ", a.ONBOARD_COMPLETE_DT \n",
    ", current_date - a.onboard_complete_dt as days_onboarded\n",
    ", a.company_sk\n",
    ", b.country_sk\n",
    ", case  when sum(input_companysize_cnt + input_functions_cnt + input_industries_cnt + input_regions_cnt + input_seniorities_cnt) >= 1 \n",
    "        then 1\n",
    "        else 0\n",
    "        end as sales_pref_set\n",
    "\n",
    "from dwh.v_dim_lss_seat a\n",
    "\n",
    "inner join dwh.v_dim_member_flat b\n",
    "on a.member_id = b.member_sk \n",
    "\n",
    "inner join dwh.v_dim_country c\n",
    "on b.country_sk = c.country_sk\n",
    "\n",
    "where a.ONBOARD_COMPLETE_DT is not null\n",
    "{0} and a.sf_contract_number in ({1})\n",
    "{2} and a.lss_contract_id IN ({3}) -- parameter\n",
    "{4} and b.country_sk in ({5})\n",
    "{6} and c.continent_sk in ({7})\n",
    "    and current_flag = 'Y'\n",
    "group by 1, 2, 3, 4, 5, 6, 7, 8\n",
    ")   \n",
    "with data primary index (lss_seat_id) \n",
    "on commit preserve rows;\"\"\".format(contract_id_filter, contract, dashboard_id_filter, dashboard, country_filter, \n",
    "                                   country, continent_filter, continent)\n",
    "\n",
    "print sql0_seats\n",
    "\n",
    "pandas.io.sql.execute(sql0_seats, cxn)\n",
    "\n",
    "#Seat Count\n",
    "\n",
    "sql0_count = \"\"\"select count(distinct member_sk) as num_sn from all_sn\"\"\"\n",
    "\n",
    "get_seat_count = pandas.read_sql(sql0_count, cxn)\n",
    "\n",
    "get_seat_count = get_seat_count.replace(np.nan, 0)\n",
    "\n",
    "get_seat_count.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql1_all_sp = \"\"\"\n",
    "create volatile table all_sp as (\n",
    "    select distinct a.member_sk\n",
    "    from dwh.v_dim_sales_member a\n",
    "        inner join dwh.v_dim_member b\n",
    "        on a.member_sk = b.member_sk\n",
    "        \n",
    "        inner join dwh.v_dim_country c\n",
    "        on b.country_sk = c.country_sk\n",
    "        \n",
    "        inner join dwh.v_dim_company d\n",
    "        on a.std_company_sk = d.company_sk\n",
    "        \n",
    "    where d.company_id in ({0})\n",
    "        {1} and c.country_sk in ({2})\n",
    "        {3} and c.continent_sk in ({4})\n",
    ")\n",
    "with data primary index (member_sk)\n",
    "on commit preserve rows\n",
    ";\n",
    "\"\"\".format(company_id, country_filter, country, continent_filter, continent)\n",
    "\n",
    "pandas.io.sql.execute(sql1_all_sp, cxn)\n",
    "\n",
    "sql1_count = \"\"\"select count(distinct member_sk) from all_sp\"\"\"\n",
    "\n",
    "get_all_sp = pandas.read_sql(sql1_count, cxn)\n",
    "\n",
    "get_all_sp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql2_combined_sp = \"\"\"\n",
    "create volatile table combined_sp as (\n",
    "    select distinct member_sk\n",
    "    from (\n",
    "        select distinct member_sk\n",
    "        from all_sn\n",
    "        union\n",
    "        select distinct member_sk\n",
    "        from all_sp\n",
    "    ) a\n",
    ")\n",
    "with data primary index (member_sk)\n",
    "on commit preserve rows\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "pandas.io.sql.execute(sql2_combined_sp, cxn)\n",
    "\n",
    "sql2_count = \"\"\"select count(distinct member_sk) as num_sp from combined_sp\"\"\"\n",
    "\n",
    "get_combined_sp = pandas.read_sql(sql2_count, cxn)\n",
    "\n",
    "num_sp = get_combined_sp[\"num_sp\"].iloc[0]\n",
    "\n",
    "print num_sp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql3_all_emp = \"\"\"\n",
    "create multiset volatile table all_emp as (\n",
    "    select p.member_sk\n",
    "    from dwh.v_dim_position p\n",
    "\n",
    "    inner join dm_stt.v_om_segment o \n",
    "        on p.member_sk = o.member_sk\n",
    "    \n",
    "    inner join dwh.v_dim_company c\n",
    "    on p.std_company_sk = c.company_sk\n",
    "\n",
    "    where p.is_final_company = 'y'\n",
    "        and p.is_final_position = 'y'\n",
    "        and p.end_date is null\n",
    "        and c.company_id in ({0})\n",
    ")\n",
    "with data primary index (member_sk) \n",
    "on commit preserve rows;\n",
    "\n",
    "\"\"\".format(company_id)\n",
    "\n",
    "print sql3_all_emp\n",
    "\n",
    "pandas.io.sql.execute(sql3_all_emp, cxn)\n",
    "\n",
    "sql3_count = \"\"\"select count(distinct member_sk) from all_sp\"\"\"\n",
    "\n",
    "get_all_emp = pandas.read_sql(sql3_count, cxn)\n",
    "\n",
    "get_all_emp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql4_tl_emp = \"\"\"\n",
    "create volatile table tl_emp as (\n",
    "    select member_sk\n",
    "    from (\n",
    "        select distinct e.member_sk\n",
    "            , count(dest_member_sk) as dm_count\n",
    "            , row_number () over (order by dm_count desc) as \"dm_conn\"\n",
    "        from all_emp e\n",
    "        inner join dwh.v_fact_detail_connections c \n",
    "        on c.source_member_sk = e.member_sk\n",
    "\n",
    "        inner join dm_stt.v_om_segment o \n",
    "        on o.member_sk = c.dest_member_sk\n",
    "\n",
    "        where o.seniority_2_sk >= 6\n",
    "            and e.member_sk not in (select distinct member_sk from all_sp)\n",
    "            and c.active ='y'\n",
    "        group by 1\n",
    "    ) a\n",
    "    where dm_conn <= 1000\n",
    ")\n",
    "with data primary index (member_sk)\n",
    "on commit preserve rows;\n",
    "\"\"\"\n",
    "\n",
    "pandas.io.sql.execute(sql4_tl_emp, cxn)\n",
    "\n",
    "sql4_count = \"\"\"select count(distinct member_sk) from tl_emp\"\"\"\n",
    "\n",
    "get_tl_emp = pandas.read_sql(sql4_count, cxn)\n",
    "\n",
    "get_tl_emp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql5_sn_cxns = \"\"\"\n",
    "create multiset volatile table sn_cxns as (\n",
    "    select c.source_member_sk as member_sk\n",
    "        , c.dest_member_sk\n",
    "        , c.date_sk\n",
    "        , b.seniority_2_sk\n",
    "    from dwh.v_fact_detail_connections c\n",
    "    inner join all_sn a \n",
    "    on c.source_member_sk = a.member_sk\n",
    "            \n",
    "    left join dm_stt.v_om_segment b\n",
    "    on c.dest_member_sk = b.member_sk\n",
    "    \n",
    "    where c.active = 'y'\n",
    "    \n",
    ") \n",
    "with data primary index (member_sk, dest_member_sk) \n",
    "on commit preserve rows\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "pandas.io.sql.execute(sql5_sn_cxns, cxn)\n",
    "\n",
    "sql5_count = \"\"\"\n",
    "select count(distinct dest_member_sk) as sn_cxns \n",
    "    , count(distinct case when a.seniority_2_sk >= 6 then dest_member_sk end) as sn_dm_cxns\n",
    "from sn_cxns a\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "get_sn_cxns = pandas.read_sql(sql5_count, cxn)\n",
    "\n",
    "get_sn_cxns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql6_com_sp_cxns = \"\"\"\n",
    "create multiset volatile table com_sp_cxns as (\n",
    "    select c.source_member_sk as member_sk\n",
    "        , c.dest_member_sk\n",
    "        , c.date_sk\n",
    "        , b.seniority_2_sk\n",
    "    from dwh.v_fact_detail_connections c\n",
    "    inner join combined_sp a \n",
    "    on c.source_member_sk = a.member_sk\n",
    "    \n",
    "    left join dm_stt.v_om_segment b\n",
    "    on c.dest_member_sk = b.member_sk\n",
    "\n",
    "    where c.active = 'y'\n",
    ") \n",
    "with data primary index (member_sk, dest_member_sk) \n",
    "on commit preserve rows\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "pandas.io.sql.execute(sql6_com_sp_cxns, cxn)\n",
    "\n",
    "sql6_count = \"\"\"\n",
    "select count(distinct dest_member_sk) as com_sp_cxns \n",
    "    , count(distinct case when a.seniority_2_sk >= 6 then dest_member_sk end) as com_sp_dm_cxns\n",
    "from com_sp_cxns a\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "sql6_count_dms = \"\"\"\n",
    "select count(distinct dest_member_sk)\n",
    "from com_sp_cxns\n",
    "where seniority_2_sk >= 6\"\"\"\n",
    "\n",
    "get_com_sp_cxns = pandas.read_sql(sql6_count, cxn)\n",
    "\n",
    "get_com_sp_cxns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql7_all_emp_cxns = \"\"\"\n",
    "create multiset volatile table all_emp_cxns as (\n",
    "    select c.source_member_sk as member_sk\n",
    "        , c.dest_member_sk\n",
    "        , c.date_sk\n",
    "        , b.seniority_2_sk\n",
    "    from dwh.v_fact_detail_connections c\n",
    "    inner join all_emp a \n",
    "    on c.source_member_sk = a.member_sk\n",
    "    \n",
    "    left join dm_stt.v_om_segment b\n",
    "    on c.dest_member_sk = b.member_sk\n",
    "    where c.active = 'y'\n",
    ") \n",
    "with data primary index (member_sk, dest_member_sk) \n",
    "on commit preserve rows\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "pandas.io.sql.execute(sql7_all_emp_cxns, cxn)\n",
    "\n",
    "sql7_count = \"\"\"\n",
    "select count(distinct dest_member_sk) as all_emp_cxns \n",
    "    , count(distinct case when a.seniority_2_sk >= 6 then dest_member_sk end) as all_emp_dm_cxns\n",
    "from all_emp_cxns a\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "get_all_emp_cxns = pandas.read_sql(sql7_count, cxn)\n",
    "\n",
    "get_all_emp_cxns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql8_tl_extend = \"\"\"\n",
    "create multiset volatile table teamlink_extend as (\n",
    "select a.member_sk\n",
    "    , a.dest_member_sk\n",
    "    , a.date_sk\n",
    "    , a.seniority_2_sk\n",
    "from com_sp_cxns a\n",
    "\n",
    "union \n",
    "\n",
    "select a.member_sk\n",
    "    , a.dest_member_sk\n",
    "    , a.date_sk\n",
    "    , a.seniority_2_sk\n",
    "from all_emp_cxns a\n",
    "\n",
    ")\n",
    "with data primary index (member_sk, dest_member_sk)\n",
    "on commit preserve rows\n",
    "\"\"\"\n",
    "\n",
    "pandas.io.sql.execute(sql8_tl_extend, cxn)\n",
    "\n",
    "sql8_count = \"\"\"\n",
    "select count(distinct dest_member_sk) as tle_cxns \n",
    "    , count(distinct case when a.seniority_2_sk >= 6 then dest_member_sk end) as tle_dm_cxns\n",
    "from teamlink_extend a\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "get_tle_cxns = pandas.read_sql(sql8_count, cxn)\n",
    "\n",
    "get_tle_cxns.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql10_avg_dm_cxns = \"\"\"\n",
    "select count(a.member_sk) / {0} as avg_dm_cxns --edit number of sales reps\n",
    "from com_sp_cxns a\n",
    "\n",
    "where a.seniority_2_sk >= 6\n",
    "\n",
    "\"\"\".format(num_sp)\n",
    "\n",
    "get_avg_dm_cxns = pandas.read_sql(sql10_avg_dm_cxns, cxn)\n",
    "\n",
    "get_avg_dm_cxns.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql11_pre_post_cxns = \"\"\"\n",
    "select 'Pre ' as time_period\n",
    "    , count(distinct dest_member_sk) as cxns\n",
    "    , dm_cxns\n",
    "from sn_cxns c\n",
    "    inner join all_sn s\n",
    "    on c.member_sk = s.member_sk\n",
    "    \n",
    "    left join (\n",
    "        select count(distinct dest_member_sk) as dm_cxns\n",
    "        from sn_cxns c\n",
    "        \n",
    "        inner join all_sn s\n",
    "        on c.member_sk = s.member_sk\n",
    "\n",
    "        inner join dm_stt.v_om_segment o\n",
    "        on c.dest_member_sk = o.member_sk\n",
    "            and o.seniority_2_sk >= 6\n",
    "        where c.date_sk between s.onboard_complete_dt - days_onboarded and s.onboard_complete_dt -1\n",
    "    ) a on 1=1\n",
    "    \n",
    "where c.date_sk between s.onboard_complete_dt - days_onboarded and s.onboard_complete_dt - 1\n",
    "\n",
    "group by 1, 3\n",
    "\n",
    "union all \n",
    "\n",
    "select 'Post'\n",
    "    , count(distinct dest_member_sk) \n",
    "    , dm_cxns\n",
    "from sn_cxns c\n",
    "    inner join all_sn s\n",
    "    on c.member_sk = s.member_sk\n",
    "    \n",
    "    left join (\n",
    "        select count(distinct dest_member_sk) as dm_cxns\n",
    "        from sn_cxns c\n",
    "        inner join all_sn s\n",
    "        on c.member_sk = s.member_sk\n",
    "        \n",
    "        inner join dm_stt.v_om_segment o\n",
    "        on c.dest_member_sk = o.member_sk\n",
    "            and o.seniority_2_sk >= 6\n",
    "        where c.date_sk >= s.onboard_complete_dt\n",
    "    ) a on 1=1\n",
    "where c.date_sk >= s.onboard_complete_dt\n",
    "group by 1, 3\n",
    "order by 1 desc\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "get_pre_post_cxns = pandas.read_sql(sql11_pre_post_cxns, cxn)\n",
    "\n",
    "get_pre_post_cxns.head()\n",
    "\n",
    "#avg_dm_cxns = get_avg_dm_cxns[\"avg_dm_cxns\"].iloc[0]\n",
    "\n",
    "#print avg_dm_cxns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql12_outbound_pvs = \"\"\"\n",
    "create volatile table outbound_pvs as (\n",
    "    select a.member_sk\n",
    "        , p.viewee_member_sk\n",
    "        , p.profile_views\n",
    "        , p.network_distance\n",
    "        , p.date_sk\n",
    "    from dwh.v_agg_dly_mmb_profile_views_vy p\n",
    "        inner join all_sn a\n",
    "            on a.member_sk = p.viewer_member_sk \n",
    "    where p.is_nonself_view = 1\n",
    ")   \n",
    "with data primary index (member_sk, viewee_member_sk) \n",
    "on commit preserve rows\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "pandas.io.sql.execute(sql12_outbound_pvs, cxn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql13_pre_post_pvs = \"\"\"\n",
    "select 'Pre ' as time_period\n",
    "    , sum(profile_views) as profile_views\n",
    "    , sum(case when o.seniority_2_sk >= 6 then profile_views end) as dm_pvs\n",
    "from outbound_pvs p\n",
    "    inner join all_sn s\n",
    "    on p.member_sk = s.member_sk\n",
    "    \n",
    "    inner join dm_stt.v_om_segment o\n",
    "    on p.viewee_member_sk = o.member_sk\n",
    "    \n",
    "where p.date_sk between s.onboard_complete_dt - days_onboarded and s.onboard_complete_dt - 1\n",
    "group by 1\n",
    "\n",
    "union all \n",
    "\n",
    "select \n",
    "    'Post'\n",
    "    , sum(profile_views) \n",
    "    , sum(case when o.seniority_2_sk >= 6 then profile_views end)\n",
    "from outbound_pvs p\n",
    "    inner join all_sn s\n",
    "    on p.member_sk = s.member_sk\n",
    "    \n",
    "    inner join dm_stt.v_om_segment o\n",
    "    on p.viewee_member_sk = o.member_sk\n",
    "    \n",
    "where p.date_sk >= s.onboard_complete_dt\n",
    "            \n",
    "group by 1\n",
    "order by 1 desc\n",
    "\n",
    "\"\"\"\n",
    "get_pre_post_pvs = pandas.read_sql(sql13_pre_post_pvs, cxn)\n",
    "\n",
    "get_pre_post_pvs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql14_pre_post_srch = \"\"\"\n",
    "select  'Pre ' as time_period\n",
    "    , sum(d.search_cnt) as searches\n",
    "from dwh.v_agg_daily_search d \n",
    "    inner join all_sn s\n",
    "    on d.member_sk = s.member_sk\n",
    "where d.date_sk between s.onboard_complete_dt - s.days_onboarded and s.onboard_complete_dt - 1\n",
    "group by 1\n",
    "\n",
    "union all\n",
    "\n",
    "select 'Post '\n",
    "    ,sn_searches\n",
    "from (\n",
    "    select sum(SN_SEARCH + LI_SEARCH_BY_SN ) \n",
    "        as sn_searches\n",
    "    from dwh.v_agg_daily_lss_seat_stats l \n",
    "        inner join all_sn s\n",
    "            on l.lss_seat_id = s.lss_seat_id\n",
    "    where l.date_sk between s.onboard_complete_dt and current_date\n",
    ") b\n",
    "group by 1\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "get_pre_post_srch = pandas.read_sql(sql14_pre_post_srch, cxn)\n",
    "\n",
    "get_pre_post_srch.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sql15_pre_temp = \"\"\"\n",
    "select count(a.member_sk) as count_sn\n",
    "    , avg(a.SCORE_SOCIAL_BRAND) as social_brand\n",
    "    , avg(a.score_right_people) as right_people\n",
    "    , avg(a.score_insights) as insights\n",
    "    , avg(a.score_relationships) as relationships\n",
    "    , avg(a.ssi) as ssi\n",
    "from dwh.v_agg_monthly_all_member_ssi a\n",
    "\n",
    "inner join all_sn b\n",
    "on a.member_sk = b.member_sk\n",
    "\n",
    "inner join dwh.v_dim_date c\n",
    "on c.month_begin_sk = a.month_begin_sk\n",
    "    and c.date_sk = (select min(b.onboard_complete_dt) from all_sn)\n",
    "\"\"\"\n",
    "\n",
    "print (sql15_pre_temp)\n",
    "get_pre_ssi_temp = pandas.read_sql(sql15_pre_temp, cxn)\n",
    "\n",
    "get_pre_ssi_temp.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sql15_pre_post_ssi = \"\"\"\n",
    "select 'Pre ' as time_period\n",
    "    , count(b.member_sk) as count_sn\n",
    "    , avg(a.pre_score_social_brand) as social_brand\n",
    "    , avg(a.pre_score_right_people) as right_people\n",
    "    , avg(a.pre_score_insights) as insights\n",
    "    , avg(a.pre_score_relationships) as relationships\n",
    "    , avg(a.pre_ssi) as ssi\n",
    "    \n",
    "from DWH.V_AGG_MBR_COMP_PRE_SSI a\n",
    "\n",
    "inner join all_sn b \n",
    "on b.member_sk = a.member_sk\n",
    "\n",
    "inner join dm_stt.v_om_segment c \n",
    "on c.member_sk = b.member_sk\n",
    "\n",
    "inner join dwh.v_dim_company d\n",
    "on c.std_company_sk = d.company_sk \n",
    "    and d.company_id in ({0})\n",
    "\n",
    "union all\n",
    "\n",
    "select 'Post' as time_period\n",
    "    , count(a.member_sk) \n",
    "    , avg(b.score_social_brand_v3) \n",
    "    , avg(b.score_right_people_v3) \n",
    "    , avg(b.score_insights_v3) \n",
    "    , avg(b.score_relationships_v3) \n",
    "    , avg(b.ssi_v3)\n",
    "\n",
    "from DWH.V_AGG_CURR_SALES_SSI b\n",
    "\n",
    "inner join all_sn a \n",
    "on b.member_sk = a.member_sk\n",
    "    \n",
    ";\n",
    "\n",
    "\"\"\".format(company_id)\n",
    "\n",
    "print (sql15_pre_post_ssi)\n",
    "get_pre_post_ssi = pandas.read_sql(sql15_pre_post_ssi, cxn)\n",
    "\n",
    "get_pre_post_ssi.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sql_15_raw_ssi = \"\"\"\n",
    "select 'Pre ' as time_period\n",
    "    , b.member_sk\n",
    "    , a.pre_score_social_brand\n",
    "    , a.pre_score_right_people\n",
    "    , a.pre_score_insights\n",
    "    , a.pre_score_relationships\n",
    "    , a.pre_ssi\n",
    "    \n",
    "from DWH.V_AGG_MBR_COMP_PRE_SSI a\n",
    "\n",
    "inner join all_sn b \n",
    "on b.member_sk = a.member_sk\n",
    "\n",
    "inner join dm_stt.v_om_segment c \n",
    "on c.member_sk = b.member_sk\n",
    "\n",
    "inner join dwh.v_dim_company d\n",
    "on c.std_company_sk = d.company_sk \n",
    "    and d.company_id in (3681456)\n",
    "\n",
    "union all\n",
    "\n",
    "select 'Post' as time_period\n",
    "    , a.member_sk\n",
    "    , b.score_social_brand_v3\n",
    "    , b.score_right_people_v3\n",
    "    , b.score_insights_v3\n",
    "    , b.score_relationships_v3\n",
    "    , b.ssi_v3\n",
    "\n",
    "from DWH.V_AGG_CURR_SALES_SSI b\n",
    "\n",
    "inner join all_sn a \n",
    "on b.member_sk = a.member_sk\n",
    "    \n",
    ";\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql16_saved_leads = \"\"\"\n",
    "select count(a.lead_member_id) as leads_saved\n",
    "\n",
    "from dwh.v_dim_lss_prospects a\n",
    "    inner join all_sn s \n",
    "       on s.lss_seat_id = a.lss_seat_id\n",
    "    inner join dm_stt.v_om_segment o\n",
    "       on a.lead_member_id = o.member_sk\n",
    "where a.active = 'Y'\n",
    "   and a.current_flag = 'Y'\n",
    "    and a.state = 'A'\n",
    "\"\"\"\n",
    "\n",
    "get_saved_leads = pandas.read_sql(sql16_saved_leads, cxn)\n",
    "\n",
    "get_saved_leads.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql17_saved_accounts = \"\"\"\n",
    "select count(c.company_sk) as accounts_saved\n",
    "from dwh.v_dim_lss_biz_prospects b\n",
    "\n",
    "inner join all_sn s\n",
    "on b.lss_seat_id = s.lss_seat_id\n",
    "\n",
    "inner join dwh.v_dim_company c\n",
    "on b.acct_company_id = c.company_id\n",
    "\n",
    "where state in ( 'A', 'I')\n",
    "    and b.active = 'Y'\n",
    "    and current_flag = 'Y'\n",
    "\"\"\"\n",
    "\n",
    "get_saved_accounts = pandas.read_sql(sql17_saved_accounts, cxn)\n",
    "\n",
    "get_saved_accounts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql18_insights = \"\"\"\n",
    "select \n",
    "    sum(a.lead_recommendations) as lead_recommendations\n",
    "    , sum(a.account_share_article_cnt) as account_shares\n",
    "    , sum(a.company_mention_news_cnt) as mentions_in_news\n",
    "    , sum(a.lead_share_article_cnt) as lead_shares\n",
    "    , sum(a.new_position_fill_cnt) as new_jobs\n",
    "    , sum(a.account_share_content_cnt) as account_content_share\n",
    "    , sum(insight_emails_sent) as insights_emails_sent\n",
    "    , sum(a.sn_search) as sn_search_only_ratio\n",
    "    , sum(a.li_search_by_sn) as li_search_only_ratio\n",
    "    , sum(a.total_profile_views) as outbound_profile_views\n",
    "\n",
    "from dwh.v_agg_daily_lss_seat_stats a\n",
    "inner join all_sn b\n",
    "on a.lss_seat_id = b.lss_seat_id\n",
    "\n",
    "where date_sk >= b.onboard_complete_dt\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "get_insights = pandas.read_sql(sql18_insights, cxn)\n",
    "\n",
    "get_insights.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Outputs\n",
    "#Connections\n",
    "avg_dm_cxns = get_avg_dm_cxns[\"avg_dm_cxns\"].iloc[0]\n",
    "sn_cxns = get_sn_cxns[\"sn_cxns\"].iloc[0]\n",
    "sn_dm_cxns = get_sn_cxns[\"sn_dm_cxns\"].iloc[0]\n",
    "\n",
    "com_sp_cxns = get_com_sp_cxns[\"com_sp_cxns\"].iloc[0]\n",
    "com_sp_dm_cxns = get_com_sp_cxns[\"com_sp_dm_cxns\"].iloc[0]\n",
    "all_emp_cxns = get_all_emp_cxns[\"all_emp_cxns\"].iloc[0]\n",
    "all_emp_dm_cxns = get_all_emp_cxns[\"all_emp_dm_cxns\"].iloc[0]\n",
    "tle_cxns = get_tle_cxns[\"tle_cxns\"].iloc[0]\n",
    "tle_dm_cxns = get_tle_cxns[\"tle_dm_cxns\"].iloc[0]\n",
    "pre_cxns = get_pre_post_cxns[\"cxns\"].iloc[0]\n",
    "pre_dm_cxns = get_pre_post_cxns[\"dm_cxns\"].iloc[0]\n",
    "post_cxns = get_pre_post_cxns[\"cxns\"].iloc[1]\n",
    "post_dm_cxns = get_pre_post_cxns[\"dm_cxns\"].iloc[1]\n",
    "dm_cxn_ratio = round(float(post_dm_cxns)/pre_dm_cxns, 1)\n",
    "cxn_ratio = round(float(post_cxns)/pre_cxns, 1)\n",
    "#Profile Views\n",
    "pre_pvs = get_pre_post_pvs[\"profile_views\"].iloc[0]\n",
    "post_pvs = get_pre_post_pvs[\"profile_views\"].iloc[1]\n",
    "pct_inc_pvs = round((float(post_pvs - pre_pvs) / float(pre_pvs))*100, 1)\n",
    "\n",
    "#Searches\n",
    "pre_srch = get_pre_post_srch[\"searches\"].iloc[0]\n",
    "post_srch = get_pre_post_srch[\"searches\"].iloc[1]\n",
    "pct_inc_srch = round((float(post_srch - pre_srch) / float(pre_srch))*100, 1)\n",
    "\n",
    "#Saved Leads & Accounts\n",
    "leads_saved = get_saved_leads[\"leads_saved\"].iloc[0]\n",
    "accounts_saved = get_saved_accounts[\"accounts_saved\"].iloc[0]\n",
    "\n",
    "#SSI\n",
    "pre_brand = get_pre_ssi_temp[\"social_brand\"].iloc[0]\n",
    "pre_people = get_pre_ssi_temp[\"right_people\"].iloc[0]\n",
    "pre_insights = get_pre_ssi_temp[\"insights\"].iloc[0]\n",
    "pre_relationships = get_pre_ssi_temp[\"relationships\"].iloc[0]\n",
    "pre_ssi = get_pre_ssi_temp[\"ssi\"].iloc[0]\n",
    "\n",
    "#pre_brand = get_pre_post_ssi[\"social_brand\"].iloc[0]\n",
    "#pre_people = get_pre_post_ssi[\"right_people\"].iloc[0]\n",
    "#pre_insights = get_pre_post_ssi[\"insights\"].iloc[0]\n",
    "#pre_relationships = get_pre_post_ssi[\"relationships\"].iloc[0]\n",
    "#pre_ssi = get_pre_post_ssi[\"ssi\"].iloc[0]\n",
    "post_brand = get_pre_post_ssi[\"social_brand\"].iloc[1]\n",
    "post_people = get_pre_post_ssi[\"right_people\"].iloc[1]\n",
    "post_insights = get_pre_post_ssi[\"insights\"].iloc[1]\n",
    "post_relationships = get_pre_post_ssi[\"relationships\"].iloc[1]\n",
    "post_ssi = get_pre_post_ssi[\"ssi\"].iloc[1]\n",
    "\n",
    "#Insights\n",
    "shares_surfaced = get_insights[\"account_shares\"].iloc[0] + get_insights[\"account_content_share\"].iloc[0] + get_insights[\"lead_shares\"].iloc[0]\n",
    "news_mentions = get_insights[\"mentions_in_news\"].iloc[0]\n",
    "insights_emails = get_insights[\"insights_emails_sent\"].iloc[0]\n",
    "\n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pptx.chart.data import ChartData\n",
    "from pptx.enum.chart import XL_CHART_TYPE\n",
    "from pptx.util import Inches\n",
    "\n",
    "\n",
    "# define chart data ---------------------\n",
    "searches_chart = ChartData()\n",
    "searches_chart.categories = get_pre_post_srch[\"time_period\"]\n",
    "#chart_data.add_series('6 Months Prior', (get_cxns_output[\"pre_target_cxns\"]))\n",
    "searches_chart.add_series('Pre to Post Searches', (get_pre_post_srch[\"searches\"]))\n",
    "\n",
    "x, y, cx, cy = Inches(1.98), Inches(1.43), Inches(6.04), Inches(3.68)\n",
    "\n",
    "# define chart data ---------------------\n",
    "pvs_chart = ChartData()\n",
    "pvs_chart.categories = get_pre_post_pvs[\"time_period\"]\n",
    "#chart_data.add_series('6 Months Prior', (get_cxns_output[\"pre_target_cxns\"]))\n",
    "pvs_chart.add_series('Pre to Post Profile Views', (get_pre_post_pvs[\"profile_views\"]))\n",
    "\n",
    "cxns_chart = ChartData()\n",
    "cxns_chart.categories = get_pre_post_cxns[\"time_period\"]\n",
    "if dm_cxn_ratio > cxn_ratio :\n",
    "    cxns_chart.add_series('Pre to Post Profile Views', (get_pre_post_cxns[\"dm_cxns\"]))\n",
    "    print \"Using DM connections as \" + str(dm_cxn_ratio) + \" is higher than \" + str(cxn_ratio)\n",
    "    str_cxn_ratio = \"Your team has accelerated their network growth by connecting to {0}X more decision makers during the pilot\".format(\n",
    "        dm_cxn_ratio)\n",
    "else :\n",
    "    cxns_chart.add_series('Pre to Post Profile Views', (get_pre_post_cxns[\"cxns\"]))\n",
    "    print \"Using all connections as \" + str(cxn_ratio) + \" is higher than \" + str(dm_cxn_ratio)\n",
    "    str_cxn_ratio = \"Your team has accelerated their network growth by connecting {0}X more during the pilot\".format(\n",
    "        cxn_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pptx import Presentation\n",
    "from pptx.enum.chart import XL_CHART_TYPE\n",
    "from pptx.util import Inches\n",
    "from pptx.enum.chart import XL_TICK_MARK\n",
    "from pptx.util import Pt\n",
    "from pptx.dml import line\n",
    "from pptx.enum.chart import XL_TICK_LABEL_POSITION\n",
    "from pptx.dml.color import RGBColor\n",
    "from pptx.enum.chart import XL_LABEL_POSITION\n",
    "import datetime\n",
    "\n",
    "\n",
    "def create_ppt(input, output):#, report_data, chart):\n",
    "    prs = Presentation(input)\n",
    "    # Use the output from analyze_ppt to understand which layouts and placeholders\n",
    "    # Create a title slide first\n",
    "    \n",
    "    #Activity Slide\n",
    "    activity_slide =  prs.slide_layouts[1]\n",
    "    \n",
    "    slide1 = prs.slides.add_slide(activity_slide)\n",
    "    title1 = slide1.shapes.title\n",
    "    subtitle1 = slide1.placeholders[10]\n",
    "    subtitle2 = slide1.placeholders[11]\n",
    "    subtitle3 = slide1.placeholders[12]\n",
    "    subtitle4 = slide1.placeholders[13]\n",
    "    subtitle5 = slide1.placeholders[14]\n",
    "    subtitle6 = slide1.placeholders[15]\n",
    "    \n",
    "    #pic = subtitle0.insert_picture(bar_img_path)\n",
    "    subtitle1.text = \"{0}\".format(post_srch)\n",
    "    subtitle2.text = \"{0}\".format(post_pvs)\n",
    "    subtitle3.text = \"{0}\".format(leads_saved)\n",
    "    subtitle4.text = \"{0}\".format(accounts_saved)\n",
    "    subtitle5.text = \"{0}\".format(post_cxns)\n",
    "    subtitle6.text = \"{0}\".format(post_dm_cxns)\n",
    "    \n",
    "    #Searches Slide\n",
    "    searches_slide = prs.slide_layouts[2]\n",
    "    slide2 = prs.slides.add_slide(searches_slide)\n",
    "    subtitle7 = slide2.placeholders[11]\n",
    "    subtitle7.text = \"Your team has accelerated their prospecting activity by performing {0}X more searches during the pilot\".format(\n",
    "        round(float(post_srch)/pre_srch, 1))\n",
    "    \n",
    "    graphic_frame = slide2.shapes.add_chart(\n",
    "    XL_CHART_TYPE.COLUMN_CLUSTERED, x, y, cx, cy, searches_chart\n",
    "    )\n",
    "    chart = graphic_frame.chart\n",
    "\n",
    "    category_axis = chart.category_axis\n",
    "    category_axis.has_major_gridlines = False \n",
    "    category_axis.has_minor_gridlines = False\n",
    "    category_axis.major_tick_mark = XL_TICK_MARK.NONE\n",
    "    category_axis.minor_tick_mark = XL_TICK_MARK.NONE\n",
    "    category_axis.tick_labels.font.size = Pt(10)\n",
    "    category_axis.tick_labels.font.color.rgb=RGBColor(0xff, 0xff, 0xff)\n",
    "    \n",
    "    value_axis = chart.value_axis\n",
    "    value_axis.minimum_scale = 0\n",
    "    value_axis.maximum_scale = None\n",
    "    value_axis.has_major_gridlines = False \n",
    "    value_axis.has_minor_gridlines = False\n",
    "    value_axis.visible = False\n",
    "    #value_axis.tick_labels.font.size = Pt(10)\n",
    "\n",
    "    plot = chart.plots[0] \n",
    "    plot.has_data_labels = True\n",
    "    data_labels = plot.data_labels\n",
    "\n",
    "    data_labels.font.size = Pt(10)\n",
    "    data_labels.font.color.rgb = RGBColor(0xff, 0xff, 0xff)\n",
    "    data_labels.position = XL_LABEL_POSITION.OUTSIDE_END\n",
    "    #tick_labels.font.color.rgb=RGBColor(0x0A, 0x42, 0x80)\n",
    "    \n",
    "    #Profile Views Slide\n",
    "    pvs_slide = prs.slide_layouts[3]\n",
    "    slide3 = prs.slides.add_slide(pvs_slide)\n",
    "    subtitle8 = slide3.placeholders[11]\n",
    "    subtitle8.text = \"Your team has accelerated their qualifying activity by viewing {0}X more profiles during the pilot\".format(\n",
    "        round(float(post_pvs)/pre_pvs, 1))\n",
    "    \n",
    "    graphic_frame = slide3.shapes.add_chart(\n",
    "    XL_CHART_TYPE.COLUMN_CLUSTERED, x, y, cx, cy, pvs_chart\n",
    "    )\n",
    "    chart = graphic_frame.chart\n",
    "\n",
    "    category_axis = chart.category_axis\n",
    "    category_axis.has_major_gridlines = False \n",
    "    category_axis.has_minor_gridlines = False\n",
    "    category_axis.major_tick_mark = XL_TICK_MARK.NONE\n",
    "    category_axis.minor_tick_mark = XL_TICK_MARK.NONE\n",
    "    category_axis.tick_labels.font.size = Pt(10)\n",
    "    category_axis.tick_labels.font.color.rgb = RGBColor(0xff, 0xff, 0xff)\n",
    "    \n",
    "    value_axis = chart.value_axis\n",
    "    value_axis.minimum_scale = 0\n",
    "    value_axis.maximum_scale = None\n",
    "    value_axis.has_major_gridlines = False \n",
    "    value_axis.has_minor_gridlines = False\n",
    "    value_axis.visible = False\n",
    "    #value_axis.tick_labels.font.size = Pt(10)\n",
    "\n",
    "    plot = chart.plots[0] \n",
    "    plot.has_data_labels = True\n",
    "    data_labels = plot.data_labels\n",
    "\n",
    "    data_labels.font.size = Pt(10)\n",
    "    data_labels.font.color.rgb = RGBColor(0xff, 0xff, 0xff)\n",
    "    data_labels.position = XL_LABEL_POSITION.OUTSIDE_END\n",
    "    #tick_labels.font.color.rgb=RGBColor(0x0A, 0x42, 0x80)\n",
    "    \n",
    "    #Connections Slide\n",
    "    cxns_slide = prs.slide_layouts[4]\n",
    "    slide4 = prs.slides.add_slide(cxns_slide)\n",
    "    subtitle9 = slide4.placeholders[11]\n",
    "    subtitle9.text = \"{0}\".format(str_cxn_ratio)\n",
    "    \n",
    "    graphic_frame = slide4.shapes.add_chart(\n",
    "    XL_CHART_TYPE.COLUMN_CLUSTERED, x, y, cx, cy, cxns_chart\n",
    "    )\n",
    "    chart = graphic_frame.chart\n",
    "\n",
    "    category_axis = chart.category_axis\n",
    "    category_axis.has_major_gridlines = False \n",
    "    category_axis.has_minor_gridlines = False\n",
    "    category_axis.major_tick_mark = XL_TICK_MARK.NONE\n",
    "    category_axis.minor_tick_mark = XL_TICK_MARK.NONE\n",
    "    category_axis.tick_labels.font.size = Pt(10)\n",
    "    \n",
    "    value_axis = chart.value_axis\n",
    "    value_axis.minimum_scale = 0\n",
    "    value_axis.maximum_scale = None\n",
    "    value_axis.has_major_gridlines = False \n",
    "    value_axis.has_minor_gridlines = False\n",
    "    value_axis.visible = False\n",
    "    #value_axis.tick_labels.font.size = Pt(10)\n",
    "\n",
    "    plot = chart.plots[0] \n",
    "    plot.has_data_labels = True\n",
    "    data_labels = plot.data_labels\n",
    "\n",
    "    data_labels.font.size = Pt(10)\n",
    "    data_labels.font.color.rgb =RGBColor(0xff, 0xff, 0xff)\n",
    "    data_labels.position = XL_LABEL_POSITION.OUTSIDE_END\n",
    "    \n",
    "    #Insights Delivered Slide\n",
    "    insights_slide = prs.slide_layouts[5]\n",
    "    slide5 = prs.slides.add_slide(insights_slide)\n",
    "    subtitle30 = slide5.placeholders[10]\n",
    "    subtitle31 = slide5.placeholders[11]\n",
    "    subtitle32 = slide5.placeholders[12]\n",
    "    subtitle33 = slide5.placeholders[13]\n",
    "\n",
    "    subtitle31.text = \"Saving leads and accounts allowed Sales Navigator to proactively keep your reps informed on the people and companies that matter to them\"\n",
    "    subtitle30.text = \"{0}\".format(shares_surfaced)\n",
    "    subtitle32.text = \"{0}\".format(news_mentions)\n",
    "    subtitle33.text = \"{0}\".format(insights_emails)\n",
    "    \n",
    "\n",
    "    network_slide = prs.slide_layouts[6]\n",
    "    slide7 = prs.slides.add_slide(network_slide)\n",
    "    subtitle20 = slide7.placeholders[11]\n",
    "    subtitle21 = slide7.placeholders[12]\n",
    "    \n",
    "    subtitle20.text = \"{0}\".format(tle_dm_cxns)\n",
    "    subtitle21.text = \"{0}\".format(avg_dm_cxns)\n",
    "\n",
    "    prs.save(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Old SSI Stuff\n",
    "\n",
    "#SSI Changes as a whole point %\n",
    "inc_social_brand = int(round(((post_brand - pre_brand) / pre_brand)*100, 1))\n",
    "inc_right_people = int(round(((post_people - pre_people) / pre_people)*100, 1))\n",
    "inc_insights = int(round(((post_insights - pre_insights) / pre_insights)*100, 1))\n",
    "inc_relationships = int(round(((post_relationships - pre_relationships) / pre_relationships)*100, 1))\n",
    "inc_ssi = int(round(((post_ssi - pre_ssi) / pre_ssi)*100, 1))\n",
    "\n",
    "if inc_social_brand > 0 :\n",
    "    inc_social_brand_text = \"{0}% increase\".format(inc_social_brand)\n",
    "elif inc_social_brand < 0:\n",
    "    inc_social_brand_text = \"{0}% decrease\".format(inc_social_brand)\n",
    "else :\n",
    "    inc_social_brand_text = \"No change\"\n",
    "    \n",
    "if inc_right_people > 0 :\n",
    "    inc_right_people_text = \"{0}% increase\".format(inc_right_people)\n",
    "elif inc_right_people < 0:\n",
    "    inc_right_people_text = \"{0}% decrease\".format(inc_right_people)\n",
    "else :\n",
    "    inc_right_people_text = \"No change\"\n",
    "    \n",
    "if inc_insights > 0 :\n",
    "    inc_insights_text = \"{0}% increase\".format(inc_insights)\n",
    "elif inc_insights < 0:\n",
    "    inc_insights_text = \"{0}% decrease\".format(inc_insights)\n",
    "else :\n",
    "    inc_insights_text = \"No change\"\n",
    "\n",
    "if inc_relationships > 0 :\n",
    "    inc_relationships_text = \"{0}% increase\".format(inc_relationships)\n",
    "elif inc_relationships < 0:\n",
    "    inc_relationships_text = \"{0}% decrease\".format(inc_relationships)\n",
    "else :\n",
    "    inc_relationships_text = \"No change\"\n",
    "\n",
    "if inc_ssi > 0 :\n",
    "    inc_ssi_text = \"{0}% increase\".format(inc_ssi)\n",
    "elif inc_ssi < 0:\n",
    "    inc_ssi_text = \"{0}% decrease\".format(inc_ssi)\n",
    "else :\n",
    "    inc_ssi_text = \"No change\"\n",
    "    \n",
    "\n",
    "print \"inc_social_brand: \" + str(inc_social_brand_text)\n",
    "print \"inc_right_people: \" + str(inc_right_people_text)\n",
    "print \"inc_insights: \" + str(inc_insights_text)\n",
    "print \"inc_relationships: \" + str(inc_relationships_text)\n",
    "print \"inc_ssi: \" + str(inc_ssi_text)\n",
    "print '\\n'\n",
    "#SSI Slide\n",
    "    ssi_slide = prs.slide_layouts[6]\n",
    "    slide6 = prs.slides.add_slide(ssi_slide)\n",
    "    subtitle23 = slide6.placeholders[20]\n",
    "    subtitle10 = slide6.placeholders[10]\n",
    "    subtitle11 = slide6.placeholders[11]\n",
    "    subtitle12 = slide6.placeholders[12]\n",
    "    subtitle13 = slide6.placeholders[13]\n",
    "    subtitle14 = slide6.placeholders[14]\n",
    "    subtitle15 = slide6.placeholders[15]\n",
    "    subtitle16 = slide6.placeholders[16]\n",
    "    subtitle17 = slide6.placeholders[17]\n",
    "    subtitle18 = slide6.placeholders[18]\n",
    "    subtitle19 = slide6.placeholders[19]\n",
    "    subtitle24 = slide6.placeholders[21]\n",
    "    subtitle25 = slide6.placeholders[22]\n",
    "    subtitle26 = slide6.placeholders[23]\n",
    "    subtitle27 = slide6.placeholders[24]\n",
    "    subtitle28 = slide6.placeholders[25]\n",
    "    subtitle29 = slide6.placeholders[26]\n",
    "    \n",
    "    subtitle23.text = \"Social Selling Adoption Metrics\"\n",
    "    subtitle10.text = \"{0}\".format(round(pre_brand, 1))\n",
    "    subtitle11.text = \"{0}\".format(round(pre_people, 1))\n",
    "    subtitle12.text = \"{0}\".format(round(pre_insights, 1))\n",
    "    subtitle13.text = \"{0}\".format(round(pre_relationships, 1))\n",
    "    subtitle14.text = \"{0}\".format(round(pre_ssi, 1))\n",
    "    subtitle15.text = \"{0}\".format(round(post_brand, 1))\n",
    "    subtitle16.text = \"{0}\".format(round(post_people, 1))\n",
    "    subtitle17.text = \"{0}\".format(round(post_insights, 1))\n",
    "    subtitle18.text = \"{0}\".format(round(post_relationships, 1))\n",
    "    subtitle19.text = \"{0}\".format(round(post_ssi, 1))\n",
    "    subtitle24.text = \"*Before pilot was a snapshot of SSI as of usersâ€™ onboard date. After pilot was a snapshot of SSI as at {0}\".format(\n",
    "        datetime.date.today())\n",
    "    subtitle25.text = \"{0}\".format(inc_social_brand_text)\n",
    "    subtitle26.text = \"{0}\".format(inc_right_people_text)\n",
    "    subtitle27.text = \"{0}\".format(inc_insights_text)\n",
    "    subtitle28.text = \"{0}\".format(inc_relationships_text)\n",
    "    subtitle29.text = \"{0}\".format(inc_ssi_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_ppt(template_location, result_location + result_filename + '.pptx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pptx import Presentation\n",
    "\n",
    "def analyze_ppt(input, output):\n",
    "    \"\"\" Take the input file and analyze the structure.\n",
    "    The output file contains marked up information to make it easier\n",
    "    for generating future powerpoint templates.\n",
    "    \"\"\"\n",
    "    prs = Presentation(input)\n",
    "    # Each powerpoint file has multiple layouts\n",
    "    # Loop through them all and  see where the various elements are\n",
    "    for index, _ in enumerate(prs.slide_layouts):\n",
    "        slide = prs.slides.add_slide(prs.slide_layouts[index])\n",
    "        # Not every slide has to have a title\n",
    "        try:\n",
    "            title = slide.shapes.title\n",
    "            title.text = 'Title for Layout {}'.format(index)\n",
    "        except AttributeError:\n",
    "            print(\"No Title for Layout {}\".format(index))\n",
    "        # Go through all the placeholders and identify them by index and type\n",
    "        for shape in slide.placeholders:\n",
    "            if shape.is_placeholder:\n",
    "                phf = shape.placeholder_format\n",
    "                # Do not overwrite the title which is just a special placeholder\n",
    "                try:\n",
    "                    if 'Title' not in shape.text:\n",
    "                        shape.text = 'Placeholder index:{} type:{}'.format(phf.idx, shape.name)\n",
    "                except AttributeError:\n",
    "                    print(\"{} has no text attribute\".format(phf.type))\n",
    "                print('{} {}'.format(phf.idx, shape.name))\n",
    "    prs.save(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#analyze_ppt('C:\\\\Users\\\\kachong\\\\Google Drive\\\\Notebooks\\\\Pilot_Review.pptx', result_location + result_filename + '.pptx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
